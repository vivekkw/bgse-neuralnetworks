{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dnmzYcTa_0A"
   },
   "source": [
    "# Introduction to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbZoP0AeEH8G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ykiGDOLbWI9"
   },
   "source": [
    "**Tensor**: Like a numpy array that can run either in CPU or GPU\n",
    "\n",
    "**Autograd**: Package for building computational graphs out of Tensors, and automatically computing gradients\n",
    "\n",
    "**Module**: A neural Network layer, may store state or learnable weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "I3y9QQEWcQLO",
    "outputId": "ee4bea98-2732-4430-ab02-4c2ce96da0f8"
   },
   "outputs": [],
   "source": [
    "#import torch library\n",
    "import torch\n",
    "\n",
    "#check if a CUDA capable GPU is available\n",
    "print(\"Cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rI6XnpvbczFp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKab311RdKXy"
   },
   "source": [
    "# Liner regression example\n",
    "\n",
    "Imagine that we have a cloud of points and want to draw a line that minimizes the square distance to all points\n",
    "\n",
    "![linear regression](https://camo.githubusercontent.com/1152fe558592a8f67ce2d590a513899fa94b7df7/687474703a2f2f7777772e61746d6f732e77617368696e67746f6e2e6564752f7e726f62776f6f642f7465616368696e672f3435312f6c6162732f696d616765732f636f6e636570747331322e6a7067)\n",
    "\n",
    "y' = w*x + b\n",
    "\n",
    "loss function = Sum( (y - y')^2 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wnWmcEfdxl2"
   },
   "source": [
    "Let's create randomly a cloud of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "RcfInApjdt0l",
    "outputId": "ab1ca07c-c427-471f-9e51-ccb74c48f57b"
   },
   "outputs": [],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "trainX = np.linspace(-1,1, 200 ) #200 evenly spaced samples\n",
    "realW=2.0\n",
    "realb=1.0\n",
    "trainY = (realW * trainX + realb + np.random.randn(*trainX.shape)*0.8)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure( figsize=(8, 8))\n",
    "plt.plot(trainX, trainY, 'bo', markeredgecolor='none')\n",
    "plt.ylabel('Line with noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "id": "cZpE_FdF-Hoc",
    "outputId": "b215cc0d-13db-4f97-c65f-7dade3986fff"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "w=torch.randn(1,device=device)\n",
    "b=torch.randn(1,device=device)\n",
    "\n",
    "X=torch.FloatTensor(trainX)\n",
    "Y=torch.FloatTensor(trainY)\n",
    "X.to(device)\n",
    "Y.to(device)\n",
    "\n",
    "print(\"X shape\", X.shape)\n",
    "print(\"Y shape\", Y.shape)\n",
    "\n",
    "learning_rate = 0.3\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    prediction = w*X + b\n",
    "    loss = (prediction - Y).pow(2).mean()\n",
    "    print(\"loss\", loss)\n",
    "\n",
    "    grad_w = (2*(prediction - Y )*X).mean()\n",
    "    grad_b = (2*(prediction - Y)).mean()\n",
    "\n",
    "    #print(grad_w.shape)\n",
    "    #print(grad_b.shape)\n",
    "\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "    print(\"w\", w, \"b\", b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kGsm2NAYBWo-",
    "outputId": "eee71ac4-6bb8-4510-c459-5806b7de3ec7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as plt\n",
    "from IPython import display\n",
    "\n",
    "fig = plt.figure( figsize=(10, 10))\n",
    "\n",
    "\n",
    "w=torch.randn(1,device=device)\n",
    "b=torch.randn(1,device=device)\n",
    "\n",
    "for epoch in range(15):\n",
    "    prediction = w*X + b\n",
    "    loss = (prediction - Y).pow(2).mean()\n",
    "    #print(\"loss\", loss)\n",
    "\n",
    "    grad_w = (2*(prediction - Y )*X).mean()\n",
    "    grad_b = (2*(prediction - Y)).mean()\n",
    "\n",
    "    #print(grad_w.shape)\n",
    "    #print(grad_b.shape)\n",
    "\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "    #print(\"w\", w, \"b\", b)\n",
    "    current_w = (w.data).cpu().numpy()\n",
    "    current_b = (b.data).cpu().numpy()\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "    plt.plot(trainX, trainX*current_w + current_b, color='r',)\n",
    "    print('Cost: ', loss,' w:',current_w, ' b:',current_b)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "print('Cost: ', loss,' w:',current_w, ' b:',current_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIbVgAKLSZNb"
   },
   "source": [
    "# Task #1\n",
    "Test last block code with different learning rates\n",
    "What happens if the learning rate is too high?\n",
    "What happens if the learning rate is too low?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jX2_ZkuLTW8O"
   },
   "source": [
    "# Autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qZdy9GD4TZ_E",
    "outputId": "200e86d7-6944-4f97-e657-a427e8dbc5f9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize=(10, 10))\n",
    "\n",
    "\n",
    "# We will not want gradients\n",
    "# (of loss) with respect to data\n",
    "X=torch.FloatTensor(trainX)\n",
    "Y=torch.FloatTensor(trainY)\n",
    "X.to(device)\n",
    "Y.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Do want gradients with\n",
    "# respect to weights \n",
    "w=torch.randn(1,device=device, requires_grad=True)\n",
    "b=torch.randn(1,device=device, requires_grad=True)\n",
    "\n",
    "for epoch in range(15):\n",
    "    prediction = w*X + b\n",
    "    loss = (prediction - Y).pow(2).mean()\n",
    "    \n",
    "    #this magically computes the gradient for all parameters involved in the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    # Make gradient step on weights, then zero\n",
    "    # them. Torch.no_grad means “don’t build\n",
    "    # a computational graph for this part\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        # PyTorch methods that end in underscore\n",
    "        # modify the Tensor in-place; methods that\n",
    "        # don’t return a new Tensor\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    #just visualization\n",
    "    current_w = (w.data).cpu().numpy()\n",
    "    current_b = (b.data).cpu().numpy()\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "    plt.plot(trainX, trainX*current_w + current_b, color='r',)\n",
    "    print('Cost: ', loss,' w:',current_w, ' b:',current_b)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t3xVpe0WqfC"
   },
   "source": [
    "# PyTorch: nn\n",
    "\n",
    "Great documentation https://pytorch.org/docs/stable/nn.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(1,1,bias=True)\n",
    "print( \"Model parameters\", list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "colab_type": "code",
    "id": "MhjkY902Wvb9",
    "outputId": "f0c8cb94-753f-4985-9da9-b5263161bdaa"
   },
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "    prediction = model(X.view(-1,1))\n",
    "    print(prediction)\n",
    "    # loss = (prediction - Y.view(-1,1)).pow(2).mean()\n",
    "    loss = torch.nn.functional.mse_loss(prediction, Y.view(-1,1))\n",
    "    print(loss)\n",
    "\n",
    "    #this magically computes the gradient for all parameters involved in the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    # Make gradient step on weights, then zero\n",
    "    # them. Torch.no_grad means “don’t build\n",
    "    # a computational graph for this part\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    #just visualization\n",
    "    params = list(model.parameters())\n",
    "    current_w = params[0].data.cpu().numpy()\n",
    "    current_b = params[1].data.cpu().numpy()\n",
    "    \n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "    plt.plot(trainX, prediction.view(-1).data.cpu().numpy(), color='r',)\n",
    "    print('Cost: ', loss.data.cpu().numpy(),' w:',current_w, ' b:',current_b)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "print('Cost: ', loss.data.cpu().numpy(),' w:',current_w, ' b:',current_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "auhmUuJbajMN",
    "outputId": "9800a7d0-3172-480c-dec9-39458d2ce7fc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyOptimizer():\n",
    "    def __init__(self,  params, lr):\n",
    "        self.learning_rate = lr\n",
    "        self.params = list(params)\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "          for param in self.params:\n",
    "              param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "          for param in self.params:\n",
    "              param.grad.zero_()\n",
    "\n",
    "\n",
    "model = torch.nn.Linear(1,1,bias=True)\n",
    "optimizer = MyOptimizer(   model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(15):\n",
    "    prediction = model(X.view(-1,1))\n",
    "    # loss = (prediction - Y.view(-1,1)).pow(2).mean()\n",
    "    loss = torch.nn.functional.mse_loss(prediction, Y.view(-1,1))\n",
    "    \n",
    "\n",
    "    #this magically computes the gradient for all parameters involved in the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #just visualization\n",
    "    params = list(model.parameters())\n",
    "    current_w = params[0].data.cpu().numpy()\n",
    "    current_b = params[1].data.cpu().numpy()\n",
    "    \n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "    plt.plot(trainX, prediction.view(-1).data.cpu().numpy(), color='r',)\n",
    "    print('Cost: ', loss.data.cpu().numpy(),' w:',current_w, ' b:',current_b)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "print('Cost: ', loss.data.cpu().numpy(),' w:',current_w, ' b:',current_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGkbfgdRUjrN"
   },
   "source": [
    "# PyTorch optimizers torch.optim\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "Nice blog post on optimizers: **An overview of gradient descent optimization algorithms**\n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "![linear regression](https://miro.medium.com/max/1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "6_OcjEB7-Yld",
    "outputId": "ea4b73fb-3928-4983-e586-4e02d22751a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyOptimizer():\n",
    "    def __init__(self,  params, lr):\n",
    "        self.learning_rate = lr\n",
    "        self.params = list(params)\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "          for param in self.params:\n",
    "              param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "          for param in self.params:\n",
    "              param.grad.zero_()\n",
    "\n",
    "\n",
    "model = torch.nn.Linear(1,1,bias=True)\n",
    "# optimizer = MyOptimizer(   model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(15):\n",
    "    prediction = model(X.view(-1,1))\n",
    "    # loss = (prediction - Y.view(-1,1)).pow(2).mean()\n",
    "    loss = torch.nn.functional.mse_loss(prediction, Y.view(-1,1))\n",
    "    \n",
    "\n",
    "    #this magically computes the gradient for all parameters involved in the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #just visualization\n",
    "    params = list(model.parameters())\n",
    "    current_w = params[0].data.cpu().numpy()\n",
    "    current_b = params[1].data.cpu().numpy()\n",
    "    \n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(trainX, trainY,'o',color='b', markeredgecolor='none')\n",
    "    plt.plot(trainX, prediction.view(-1).data.cpu().numpy(), color='r',)\n",
    "    print('Cost: ', loss.data.cpu().numpy(),' w:',current_w, ' b:',current_b)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "print('Cost: ', loss.data.cpu().numpy(),' w:',current_w, ' b:',current_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HkLypz-fXbi"
   },
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuNZjxYsfOJv"
   },
   "source": [
    "<img src=\"http://diffsharp.github.io/DiffSharp/img/examples-neuralnetworks-neuron.png\" alt=\"Single Neuron Logistic Function\" style=\"width: 400px;\">\n",
    "\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg style=\"width: 300px;\" ALIGN=\"right\" >\n",
    "Sigmoid function: $$\\sigma ( z ) = \\frac{1}{1+e^{-z}}$$ \n",
    "\n",
    "Output a: $$ \\hat{y} = \\sigma(  \\sum_{i=0}^{n}{x_{i} \\cdot w_{i}} + b )$$ \n",
    "$$ \\hat{y} = \\sigma( {\\bf x} \\cdot {\\bf W} + b) $$\n",
    "\n",
    "where $ {\\bf x} = \\begin{pmatrix} x_1 \\cdots  x_n  \\end{pmatrix} $\n",
    "\n",
    "and $  {\\bf W} = \\begin{pmatrix} w_1 \\cdots  w_n  \\end{pmatrix}^T $\n",
    "\n",
    "\n",
    "PyTorch Loss functions:\n",
    "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2328/1*n1T0iYxmckzLGMMpRH6TuA.png\" alt=\"Binary Cross Entropy Loss\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nqidv55Rfm6E"
   },
   "source": [
    "lets create our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(Y*torch.log(y.pred)+((1-Y)*torch.log(1-y.pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "colab_type": "code",
    "id": "uhlFb1y1fN1-",
    "outputId": "90135467-ca55-460b-bb78-23a6383bd1a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "XX1 = np.concatenate( (np.random.randn( 500,2) + [2,3], \n",
    "                       np.random.randn( 500,2) + [0,-1]), axis=0)\n",
    "YY1 = np.concatenate( (np.zeros((500,1)), np.ones((500,1))), axis=0)\n",
    "indexes=np.arange(1000)\n",
    "np.random.shuffle(indexes)\n",
    "YY1=YY1[indexes,:]#.astype(np.float)\n",
    "XX1=XX1[indexes,:]#.astype(np.float)\n",
    "print(XX1.shape, YY1.shape)\n",
    "fig = plt.figure( figsize=(8, 8))\n",
    "plt.scatter(XX1[:, 0], XX1[:, 1], marker='o', c=YY1[:,0], s=50, lw = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "eXxWdLQdfLnd",
    "outputId": "1491d9ee-1b75-41ba-9d0c-45fe42ca0f51"
   },
   "outputs": [],
   "source": [
    "class MyLogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super(MyLogisticRegression, self).__init__()\n",
    "\n",
    "        #self.linear1 = torch.nn.Linear(dim_in, 1)\n",
    "        #self.act1 = torch.nn.Sigmoid()\n",
    "\n",
    "        #torch.nn.Sequential applies layers to input sequentially\n",
    "        self.classifier = torch.nn.Sequential( \n",
    "          torch.nn.Linear(dim_in, 1),\n",
    "          torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.loss_function = torch.nn.BCELoss()\n",
    "\n",
    "    # Not necessary to define backward, Autograd takes care of it\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.act1(x)\n",
    "        #return x\n",
    "        return  self.classifier(x)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        return self.loss_function(x,y)\n",
    "\n",
    "    def accuracy(self, predictions, y_true):\n",
    "        y_pred = (predictions > 0.5).float()\n",
    "        acc_pred = (y_pred == y_true).float().mean()\n",
    "        return acc_pred * 100\n",
    "\n",
    "\n",
    "model = MyLogisticRegression(dim_in = 2)\n",
    "learning_rate=0.3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "X=torch.FloatTensor(XX1)\n",
    "Y=torch.FloatTensor(YY1)\n",
    "X.to(device)\n",
    "Y.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    y_pred = model(X)\n",
    "    loss = model.loss(y_pred, Y)\n",
    "#     loss = -1*torch.mean(Y*torch.log(y_pred)+((1-Y)*torch.log(1-y_pred)))\n",
    "\n",
    "    #this magically computes the gradient for all parameters involved in the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #just visualization\n",
    "    print('Loss: ', loss.data.cpu().numpy())\n",
    "\n",
    "y_pred = model(X)\n",
    "print('Training accuracy:', model.accuracy(y_pred,Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6q7nky6TUvq"
   },
   "source": [
    "# Task 2\n",
    "\n",
    "Write loss code in pytorch in previous code.\n",
    "\n",
    "Hints:\n",
    "\n",
    "*   log -> torch.log()\n",
    "*   sum -> toch.sum()\n",
    "*   mean -> torch.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -1*torch.mean(Y*torch.log(y_pred)+((1-Y)*torch.log(1-y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dwe7wRazkCv6"
   },
   "source": [
    "#### Decision boundary\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg style=\"width: 300px;\" ALIGN=\"right\" >\n",
    "\n",
    "$ sigmoid(z) = 0.5$\n",
    "\n",
    "which means that:\n",
    "\n",
    "$z=0.0$\n",
    "\n",
    "In our regression model we have:\n",
    "\n",
    "$z = x1 \\cdot w1 + x2 \\cdot w2 + b = 0$\n",
    "\n",
    "We can draw the line in the x1 , x2 plane:\n",
    "\n",
    "$x2 = (-b -x1 \\cdot w1)/w2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "colab_type": "code",
    "id": "x9TnMDS-nQdI",
    "outputId": "329dda0b-efe7-4611-d592-693e701b0038"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "x1= np.linspace(-4,6, 200 )\n",
    "fig = plt.figure( figsize=(8, 8))\n",
    "\n",
    "\n",
    "model = MyLogisticRegression(dim_in = 2)\n",
    "learning_rate=0.3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    y_pred = model(X)\n",
    "    # loss = (prediction - Y.view(-1,1)).pow(2).mean()\n",
    "    loss = model.loss(y_pred, Y)\n",
    "    \n",
    "\n",
    "    #this magically computes the gradient for all parameters involved in the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #just visualization\n",
    "    params = list( model.parameters())\n",
    "    current_W = params[0].data.cpu().numpy()[0]\n",
    "    current_b = params[1].data.cpu().numpy()\n",
    "\n",
    "    x2 = (-current_b -x1*current_W[0])/current_W[1]\n",
    "    plt.clf()\n",
    "    plt.scatter(XX1[:, 0], XX1[:, 1], marker='o', c=YY1[:,0], s=50, lw = 0)\n",
    "    plt.plot(x1,x2, \"g--\", lw=4)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VEcdThsz2Gs"
   },
   "source": [
    "# Multi-class Classification Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3mebhAL0ctE"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/segurac/deeplearning-tutorials/dece1ad401a91fd3f7db1aa48a8ebfc6da2841e5/intro_theano/2layersGeneric.png\" alt=\"Multiple classes\" style=\"width: 500px;\" ALIGN=\"right\" >\n",
    "Layer outputs:\n",
    "\n",
    "$${\\bf x}^2 = h^1(  {\\bf x}^1 \\cdot {\\bf W}^1 + {\\bf b}^1 )  $$\n",
    "\n",
    "$${\\bf x}^3 = h^2(  {\\bf x}^2 \\cdot {\\bf W}^2 + {\\bf b}^2 )  $$\n",
    "\n",
    "where $ {\\bf x} = \\begin{pmatrix} x_1 \\cdots  x_n  \\end{pmatrix} $ \n",
    "\n",
    "$ {\\bf b} = \\begin{pmatrix} b_1 \\cdots  b_m  \\end{pmatrix} $ \n",
    "\n",
    "$  {\\bf W} = \\begin{pmatrix} w_{1,1} \\cdots  w_{1,m} \\\\ \n",
    "\\vdots \\cdots  \\vdots \\\\\n",
    "w_{n,1} \\cdots w_{n,m} \\\\\n",
    "\\end{pmatrix} $\n",
    "\n",
    "$ h(  ) $ is the activation function (ReLU, sigmoid, Softmax etc)\n",
    "\n",
    "For Multi-class classification: $h^2( \\cdot ) = softmax$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYlgor3-jDYi"
   },
   "source": [
    "# MNIST data\n",
    "<img src=\"https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "gNZu8vav0rIa",
    "outputId": "8fedf8eb-7ef5-4521-bfed-eb63fdef43cb"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "base_dir = '/content/sample_data/'\n",
    "train = pd.read_csv(base_dir + 'mnist_train_small.csv',header=None)\n",
    "test = pd.read_csv(base_dir + 'mnist_test.csv', header=None)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "SY8yk2Bz30V7",
    "outputId": "2fb09b69-7e4a-4733-cb1b-2c926d411cc8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Convert Dataframe into format ready for training\n",
    "def createImageData(raw: pd.DataFrame):\n",
    "    y = raw.iloc[:,0].values\n",
    "    y.resize(y.shape[0],1)\n",
    "    x = raw.iloc[:,1:].values\n",
    "    x = x.reshape([-1,1, 28, 28])\n",
    "    y = y.astype(int).reshape(-1)\n",
    "    x = x.astype(float)\n",
    "    return x, y\n",
    "## Convert to One Hot Encoding\n",
    "def one_hot_embedding(labels, num_classes=10):\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels]\n",
    "x_train, y_train = createImageData(train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "x_test, y_test = createImageData(test)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2vXREmEN45wD",
    "outputId": "5c3e2eae-eed7-4e60-dfe0-58f75adc3628"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JanuRZEH0-5i"
   },
   "source": [
    "# MNIST visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "ND8rHlyA09IC",
    "outputId": "e0d17a8a-836f-4937-9bc9-4f443f60f627"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "show_data = np.copy(x_test[0:16,:])\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10,10))\n",
    "vmin, vmax = show_data.min(), show_data.max()\n",
    "for coef, ax in zip(show_data, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "           vmax=.5 * vmax)\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "print(y_test[0:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abqG-FQouGdr"
   },
   "source": [
    "# Data normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jJRWt1dluKXP",
    "outputId": "3cad8249-ebfe-42b0-ceb3-646d3919933f"
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "m_train = x_train.mean()\n",
    "std_train = x_train.std()\n",
    "\n",
    "x_train = (x_train - m_train) / std_train\n",
    "x_test = (x_test - m_train) / std_train\n",
    "print(x_train.mean(), x_train.std(), x_test.mean(), x_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNY8F24S6ozE"
   },
   "source": [
    "# Define our MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mzBpRhSj6rZ2",
    "outputId": "0f132ffd-bdc4-4562-ce9d-17350d2a7522"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, dim_in, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential( \n",
    "          nn.Linear(dim_in, 128),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(128, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Not necessary to define backward, Autograd takes care of it\n",
    "    def forward(self, x):\n",
    "        return  self.classifier(x)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        return self.loss_function(x,y)\n",
    "\n",
    "    def raw_to_probs(self, raw):\n",
    "        return torch.nn.functional.softmax(raw)\n",
    "\n",
    "    def accuracy(self, predictions, y_true):\n",
    "        y_pred = predictions.argmax(dim=1)\n",
    "        #print(y_pred.cpu().numpy())\n",
    "        acc_pred = (y_pred == y_true).float().mean()\n",
    "        return acc_pred * 100\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = MLPClassifier(28*28, 10)\n",
    "model.to(device)\n",
    "print(model)\n",
    "summary(model, input_size=(1, 28*28))\n",
    "learning_rate = 0.3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "mbatch_size = 50\n",
    "n_epochs = 50\n",
    "n_batches_train = int(x_train.shape[0]/mbatch_size)\n",
    "\n",
    "test_data = torch.FloatTensor(x_test).view(-1,28*28).to(device)\n",
    "test_label = torch.LongTensor(y_test).view(-1).to(device)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accs = []\n",
    "test_accs= []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(n_batches_train):\n",
    "        start_idx = mbatch_size*i\n",
    "        end_idx = mbatch_size*(i+1)\n",
    "        data = x_train[start_idx:end_idx,:] \n",
    "        label = y_train[start_idx:end_idx]\n",
    "        data = torch.FloatTensor(data).view(-1,28*28).to(device)\n",
    "        label = torch.LongTensor(label).view(-1).to(device)\n",
    "        # print(data.shape, label.shape)\n",
    "\n",
    "        raw_preds = model(data)\n",
    "        loss = model.loss(raw_preds, label)\n",
    "        if i%100 == 0:\n",
    "            with torch.no_grad():\n",
    "#                print(\"Training loss:\", loss.cpu().item())\n",
    "                preds = model.raw_to_probs(raw_preds)\n",
    "                acc = model.accuracy( preds , label ).cpu().item()\n",
    "#                print(\"Training accuracy:\", ) \n",
    "                train_losses.append(loss.cpu().item())\n",
    "                train_accs.append(acc)\n",
    "\n",
    "            \n",
    "                test_raw_preds = model(test_data)\n",
    "                test_loss = model.loss(test_raw_preds, test_label)\n",
    "#                print(\"Testing loss:\", test_loss.cpu().item())\n",
    "                test_preds = model.raw_to_probs(test_raw_preds)\n",
    "                test_acc = model.accuracy( test_preds , test_label ).cpu().item()\n",
    "#                print(\"Testing accuracy:\", test_acc)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accs.append(test_acc)\n",
    "\n",
    "        #this magically computes the gradient for all parameters involved in the loss function\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model.raw_to_probs(raw_preds)\n",
    "        acc = model.accuracy( preds , label ).cpu().item()\n",
    "\n",
    "        model.eval()   \n",
    "        test_raw_preds = model(test_data)\n",
    "        test_loss = model.loss(test_raw_preds, test_label)\n",
    "        test_preds = model.raw_to_probs(test_raw_preds)\n",
    "        test_acc = model.accuracy( test_preds , test_label ).cpu().item()  \n",
    "        print(\"Epoch:\", epoch, \"Training/Testing Loss:\", loss.cpu().item(), \" / \", test_loss.cpu().item(), \" Training/Testing Acc:\", acc, \" / \", test_acc)\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_0yebo3pFqO2",
    "outputId": "82c01d0a-0784-4e7c-ec4b-5e825f090bcd"
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize=(15, 15))\n",
    "plt.plot(train_losses, '-b', label='train loss')\n",
    "plt.plot(test_losses, '-r', label='test loss')\n",
    "\n",
    "plt.xlabel(\"n iter * 100\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure( figsize=(15, 15))\n",
    "plt.plot(train_accs, '-b', label='train accuracy')\n",
    "plt.plot(test_accs, '-r', label='test accuracy')\n",
    "\n",
    "plt.xlabel(\"n iter * 100\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "# save image\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZ49SWhclA5C"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
