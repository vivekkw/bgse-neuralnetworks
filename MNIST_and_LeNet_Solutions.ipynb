{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxHt4FLCzfk3"
   },
   "source": [
    "# MNIST and LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "PEwovmeezc2d",
    "outputId": "79c9f1ac-3df4-4100-c883-4d3eee5b01a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting poutyne\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/3e/f1bfaa10e802f47496b1e326180f810b1708442b8c9bf7315faf3d267118/Poutyne-0.6-py3-none-any.whl (83kB)\n",
      "\r",
      "\u001b[K     |████                            | 10kB 31.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 20kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 30kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 40kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 51kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 61kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 71kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 81kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 92kB 3.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.17.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.3.1)\n",
      "Installing collected packages: poutyne\n",
      "Successfully installed poutyne-0.6\n",
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "!pip install poutyne\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKQeIeDDRf-n"
   },
   "source": [
    "# PyTorch Datasets and Dataloaders\n",
    "\n",
    "Datasets documentation:\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "Dataloaders\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "\n",
    "TorchVision datasets:\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n",
    "\n",
    "\n",
    "torchvision transforms:\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html#generic-transforms\n",
    "\n",
    "Example of data augmentation using transforms:\n",
    "\n",
    "<code>\n",
    "composed = transforms.Compose([\n",
    "        Rescale(256),\n",
    "        RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "        ])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "ZvNuVxNYzw8k",
    "outputId": "d8cdc329-627d-4bbd-810f-987e3ad7b0d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:00, 18532034.36it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 300106.67it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 5243217.45it/s]                           \n",
      "8192it [00:00, 126734.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Normalization, data transformation can also be used for data augmentation\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = datasets.MNIST(root = './data', train = True, transform = transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "\n",
    "test_set = datasets.MNIST(root='./data', train = False, download = True, transform = transform)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=dataset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True, num_workers=2)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                dataset=val_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXGzeR2RUNIl"
   },
   "source": [
    "Same MLPClassifier we saw last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaoPJ1GVDot4"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, dim_in=28*28, num_classes=10):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential( \n",
    "          nn.Linear(dim_in, 128),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(128, num_classes),\n",
    "        )\n",
    "        \n",
    "    # Not necessary to define backward, Autograd takes care of it\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return  self.classifier(x)\n",
    "\n",
    "\n",
    "    def raw_to_probs(self, raw):\n",
    "        return torch.nn.functional.softmax(raw)\n",
    "\n",
    "    def accuracy(self, predictions, y_true):\n",
    "        y_pred = predictions.argmax(dim=1)\n",
    "        #print(y_pred.cpu().numpy())\n",
    "        acc_pred = (y_pred == y_true).float().mean()\n",
    "        return acc_pred * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E11KB3YKUTrg"
   },
   "source": [
    "# Task\n",
    "\n",
    "Write a LeNet-like Convolutional Neural Network\n",
    "<img src=\"https://www.researchgate.net/profile/Yiren_Zhou/publication/312170477/figure/fig1/AS:448817725218816@1484017892071/Structure-of-LeNet-5.png\">\n",
    "\n",
    "Where it should have \n",
    "1.   2D Conv layer of 20 filters of 5x5 https://pytorch.org/docs/stable/nn.html#conv2d\n",
    "2.   ReLU https://pytorch.org/docs/stable/nn.html#relu\n",
    "3.   MaxPooling2D https://pytorch.org/docs/stable/nn.html#maxpool2d\n",
    "4.   2D Conv layer of 50 filters of 5x5\n",
    "5.   ReLU\n",
    "6.   MaxPooling2D\n",
    "7.   Flatten https://pytorch.org/docs/stable/nn.html#flatten\n",
    "8.   Linear layer with 500 outputs https://pytorch.org/docs/stable/nn.html#linear\n",
    "9.   ReLU\n",
    "10.  Linear leyer with 10 output classes\n",
    "\n",
    "It is easier to group convolutions, relus and maxpoolings using nn.Sequential https://pytorch.org/docs/stable/nn.html#sequential as a trainable feature extractor, and same for linear layers as the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNt-lEuk0zDi"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.featurizer = nn.Sequential( #input image 1 channel 28 pixels width and 28 pixels height 1x28x28\n",
    "            nn.Conv2d(1, 20, 5),  #output shape is 20x24x24\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), #output shape is 20x12x12\n",
    "            nn.Conv2d(20, 50, 5), #output shape is 50x8x8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), #output shape is 50x4x4\n",
    "            nn.Flatten() #output shape is 1x800  (50*4*4)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*4*50, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def raw_to_probs(self, raw):\n",
    "        return torch.nn.functional.softmax(raw)\n",
    "\n",
    "    def accuracy(self, predictions, y_true):\n",
    "        y_pred = predictions.argmax(dim=1)\n",
    "        acc_pred = (y_pred == y_true).float().mean()\n",
    "        return acc_pred * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "colab_type": "code",
    "id": "SMZGGOrpFa-d",
    "outputId": "71de2d71-8bc3-4244-c97b-930e657fa31f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (featurizer): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 20, 24, 24]             520\n",
      "              ReLU-2           [-1, 20, 24, 24]               0\n",
      "         MaxPool2d-3           [-1, 20, 12, 12]               0\n",
      "            Conv2d-4             [-1, 50, 8, 8]          25,050\n",
      "              ReLU-5             [-1, 50, 8, 8]               0\n",
      "         MaxPool2d-6             [-1, 50, 4, 4]               0\n",
      "           Flatten-7                  [-1, 800]               0\n",
      "            Linear-8                  [-1, 500]         400,500\n",
      "              ReLU-9                  [-1, 500]               0\n",
      "           Linear-10                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.27\n",
      "Params size (MB): 1.64\n",
      "Estimated Total Size (MB): 1.91\n",
      "----------------------------------------------------------------\n",
      "Epoch 1/10 17.93s Step 1200/1200: loss: 0.185896, acc: 94.590000, fscore_micro: 0.945900, val_loss: 0.077900, val_acc: 97.750000, val_fscore_micro: 0.977500\n",
      "Epoch 2/10 17.72s Step 1200/1200: loss: 0.124306, acc: 96.668333, fscore_micro: 0.966683, val_loss: 0.111705, val_acc: 96.990000, val_fscore_micro: 0.969900\n",
      "Epoch 3/10 17.84s Step 1200/1200: loss: 0.105930, acc: 97.148333, fscore_micro: 0.971483, val_loss: 0.084804, val_acc: 97.500000, val_fscore_micro: 0.975000\n",
      "Epoch 4/10 17.82s Step 1200/1200: loss: 0.101411, acc: 97.348333, fscore_micro: 0.973483, val_loss: 0.085137, val_acc: 97.740000, val_fscore_micro: 0.977400\n",
      "Epoch 5/10 17.88s Step 1200/1200: loss: 0.095395, acc: 97.591667, fscore_micro: 0.975917, val_loss: 0.079423, val_acc: 97.750000, val_fscore_micro: 0.977500\n",
      "Epoch 6/10 17.69s Step 1200/1200: loss: 0.102213, acc: 97.526667, fscore_micro: 0.975267, val_loss: 0.066703, val_acc: 98.220000, val_fscore_micro: 0.982200\n",
      "Epoch 7/10 18.33s Step 1200/1200: loss: 0.097264, acc: 97.663333, fscore_micro: 0.976633, val_loss: 0.082421, val_acc: 98.050000, val_fscore_micro: 0.980500\n",
      "Epoch 8/10 ETA 10s Step 453/1200: loss: 0.373904, acc: 94.000000"
     ]
    }
   ],
   "source": [
    "from poutyne.framework import Model\n",
    "                      \n",
    "#mymodel = MLPClassifier()\n",
    "mymodel = LeNet()\n",
    "mymodel = mymodel.to(device)\n",
    "print(mymodel)\n",
    "summary(mymodel, input_size=(1, 28, 28)) \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam( filter(lambda p: p.requires_grad, mymodel.parameters()), lr=learning_rate )\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(mymodel, optimizer, loss_function, batch_metrics=['accuracy'], epoch_metrics=['f1'])\n",
    "\n",
    "# Send model on GPU\n",
    "model.to(device)\n",
    "\n",
    "model.fit_generator(train_loader, val_loader, epochs=10)\n",
    "\n",
    "\n",
    " # Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cncigq4O4tgb"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion, acc_metric):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    n_iter = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data, label = data\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "            raw_preds = model(data)\n",
    "            loss = criterion(raw_preds, label).cpu().item()\n",
    "            acc = acc_metric( raw_preds , label ).cpu().item()\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "            n_iter += 1\n",
    "\n",
    "\n",
    "    total_loss /= n_iter\n",
    "    total_acc /= n_iter\n",
    "    model.train()\n",
    "    return total_loss, total_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s1M9kgJy1eff",
    "outputId": "ae75d0c1-d42a-41a1-9657-a7d54f1836da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (featurizer): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 20, 24, 24]             520\n",
      "              ReLU-2           [-1, 20, 24, 24]               0\n",
      "         MaxPool2d-3           [-1, 20, 12, 12]               0\n",
      "            Conv2d-4             [-1, 50, 8, 8]          25,050\n",
      "              ReLU-5             [-1, 50, 8, 8]               0\n",
      "         MaxPool2d-6             [-1, 50, 4, 4]               0\n",
      "           Flatten-7                  [-1, 800]               0\n",
      "            Linear-8                  [-1, 500]         400,500\n",
      "              ReLU-9                  [-1, 500]               0\n",
      "           Linear-10                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.27\n",
      "Params size (MB): 1.64\n",
      "Estimated Total Size (MB): 1.91\n",
      "----------------------------------------------------------------\n",
      " Batch: 1180 Training: 0.027311038225889206  /   Training Acc: 100.0\n",
      " Epoch: 0 Training/Validation Loss: 0.30652750508510507  /  0.10209415680728853  Training/Valitaion Acc: 91.02333066225052  /  96.959997215271\n",
      " Batch: 1180 Training: 0.09163318574428558  /   Training Acc: 96.0\n",
      " Epoch: 1 Training/Validation Loss: 0.07329785169606717  /  0.05604371145134792  Training/Valitaion Acc: 97.72499723434449  /  98.28999717712402\n",
      " Batch: 1180 Training: 0.015602979809045792  /   Training Acc: 100.0\n",
      " Epoch: 2 Training/Validation Loss: 0.051850765389584316  /  0.04563311804784462  Training/Valitaion Acc: 98.40666372299194  /  98.66999736785888\n",
      " Batch: 1180 Training: 0.07145582139492035  /   Training Acc: 97.99999237060547\n",
      " Epoch: 3 Training/Validation Loss: 0.040530932156931765  /  0.03814728679484688  Training/Valitaion Acc: 98.72499724706014  /  98.76999698638916\n",
      " Batch: 1180 Training: 0.1622055023908615  /   Training Acc: 94.0\n",
      " Epoch: 4 Training/Validation Loss: 0.033721264121559215  /  0.026493383849738164  Training/Valitaion Acc: 98.9149976348877  /  99.14999797821045\n",
      " Batch: 1180 Training: 0.139773890376091  /   Training Acc: 97.99999237060547\n",
      " Epoch: 5 Training/Validation Loss: 0.028026320835197113  /  0.02189562532497803  Training/Valitaion Acc: 99.11166456858317  /  99.31999813079834\n",
      " Batch: 1180 Training: 0.00875329039990902  /   Training Acc: 100.0\n",
      " Epoch: 6 Training/Validation Loss: 0.024362847817489333  /  0.017027671900577843  Training/Valitaion Acc: 99.23166467030843  /  99.51999862670898\n",
      " Batch: 1180 Training: 0.01269521750509739  /   Training Acc: 100.0\n",
      " Epoch: 7 Training/Validation Loss: 0.02039495405755588  /  0.01571281417738646  Training/Valitaion Acc: 99.38166496912639  /  99.55999847412109\n",
      " Batch: 1180 Training: 0.010429363697767258  /   Training Acc: 100.0\n",
      " Epoch: 8 Training/Validation Loss: 0.017368588434874255  /  0.01540093769915984  Training/Valitaion Acc: 99.46333176930746  /  99.52999858856201\n",
      " Batch: 1180 Training: 0.038630448281764984  /   Training Acc: 97.99999237060547\n",
      " Epoch: 9 Training/Validation Loss: 0.015287475936408251  /  0.012578477107090293  Training/Valitaion Acc: 99.55999856948853  /  99.70999889373779\n",
      " Batch: 1180 Training: 0.008523578755557537  /   Training Acc: 100.0\n",
      " Epoch: 10 Training/Validation Loss: 0.013105701672702707  /  0.014524145062896423  Training/Valitaion Acc: 99.60666538238526  /  99.53999870300294\n",
      " Batch: 1180 Training: 0.003198604565113783  /   Training Acc: 100.0\n",
      " Epoch: 11 Training/Validation Loss: 0.011132700151580745  /  0.010137115081706725  Training/Valitaion Acc: 99.68833232243855  /  99.6899988937378\n",
      " Batch: 1180 Training: 0.0005074691725894809  /   Training Acc: 100.0\n",
      " Epoch: 12 Training/Validation Loss: 0.010160310193865978  /  0.010440670837051585  Training/Valitaion Acc: 99.6983323097229  /  99.72999904632569\n",
      " Batch: 1180 Training: 0.0008797264308668673  /   Training Acc: 100.0\n",
      " Epoch: 13 Training/Validation Loss: 0.008735079499477555  /  0.006820220286390395  Training/Valitaion Acc: 99.74833244959514  /  99.85999946594238\n",
      " Batch: 1180 Training: 0.010490532033145428  /   Training Acc: 100.0\n",
      " Epoch: 14 Training/Validation Loss: 0.0072453419157318425  /  0.005274245119071565  Training/Valitaion Acc: 99.80833267847697  /  99.8899995803833\n",
      "Testing Loss: 0.022939903693187488  Testing Acc: 99.27999824523926\n",
      "Testing Loss Best Model: 0.022939903693187488  Testing Acc Best Model: 99.27999824523926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 15\n",
    "model = LeNet()\n",
    "#model =MLPClassifier()\n",
    "model.to(device)\n",
    "print(model)\n",
    "summary(model, input_size=(1, 28,28))\n",
    "learning_rate = 0.03\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "prev_loss = None\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_losses =[]\n",
    "    train_accs = []\n",
    "\n",
    "    for i, data in enumerate( train_loader):\n",
    "        data, label = data\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        raw_preds = model(data)\n",
    "        loss = criterion(raw_preds, label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss.cpu().item()\n",
    "        acc = model.accuracy( raw_preds , label ).cpu().item()\n",
    "        train_losses.append(loss)\n",
    "        train_accs .append(acc)\n",
    "        if i%20 == 0:\n",
    "            print(\"\\r\", \"Batch:\", i, \"Training:\", loss, \" / \", \" Training Acc:\", acc, end=\"\")            \n",
    "\n",
    "\n",
    "    #Print losses and metrics every epoch\n",
    "    train_loss = np.mean(np.asarray(train_losses))\n",
    "    train_acc = np.mean(np.asarray(train_accs))\n",
    "    val_loss, val_acc = eval_model(model, val_loader, criterion, model.accuracy)\n",
    "    print(\"\\n\", \"Epoch:\", epoch, \"Training/Validation Loss:\", train_loss, \" / \", val_loss, \" Training/Valitaion Acc:\", train_acc, \" / \", val_acc)\n",
    "\n",
    "    if prev_loss is None or val_loss < prev_loss:\n",
    "        prev_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "test_loss, test_acc = eval_model(model, test_loader, criterion, model.accuracy)\n",
    "print(\"Testing Loss:\", test_loss, \" Testing Acc:\", test_acc)\n",
    "test_loss, test_acc = eval_model(best_model, test_loader, criterion, model.accuracy)\n",
    "print(\"Testing Loss Best Model:\", test_loss, \" Testing Acc Best Model:\", test_acc)\n",
    "\n",
    "torch.save(best_model.state_dict(), \"./best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wKIe0W5zJpyF",
    "outputId": "db01e088-e63e-487a-bd3d-641cfdbda657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss Best Model: 0.02293997878149753  Testing Acc Best Model: 99.28\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "model.load_state_dict(torch.load(\"./best_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "test_loss, test_acc = eval_model(model, test_loader, criterion, model.accuracy)\n",
    "print(\"Testing Loss Best Model:\", test_loss, \" Testing Acc Best Model:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MNIST and LeNet Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
